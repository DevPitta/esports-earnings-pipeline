{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "import urllib3\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/06 17:31:31 WARN Utils: Your hostname, pitta resolves to a loopback address: 127.0.1.1; using 192.168.100.7 instead (on interface enp6s0)\n",
      "24/01/06 17:31:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/06 17:31:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"esport_data_bronze\") \\\n",
    "    .config(\"spark.executor.memory\", \"64g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Recent Tournaments Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable warnings\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "# Set your API key and the API endpoint URL\n",
    "api_endpoint = \"http://api.esportsearnings.com/v0/LookupRecentTournaments\"\n",
    "\n",
    "# Load the offset from a file, or start from 0 if it doesn't exist\n",
    "try:\n",
    "    with open(\"offset.txt\", \"r\") as offset_file:\n",
    "        offset = int(offset_file.read())\n",
    "except FileNotFoundError:\n",
    "    offset = 0\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# Initialize parameters\n",
    "batch_size = 100\n",
    "max_retries = 5\n",
    "retries = 0  # Initialize the retry counter\n",
    "\n",
    "while retries < max_retries:\n",
    "    # Set up the request parameters\n",
    "    params = {\n",
    "        \"apikey\": api_key,\n",
    "        \"offset\": offset,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Make the API request\n",
    "        response = requests.get(api_endpoint, params=params, verify=False)\n",
    "        \n",
    "        # Check for successful response\n",
    "        if response.status_code == 200:\n",
    "            # Check if response content is b'' (empty bytes)\n",
    "            if response.content == b'':\n",
    "                print(\"No more data to retrieve\")\n",
    "                break\n",
    "            data = response.json()\n",
    "            # Check if data is empty\n",
    "            if not data or data == b'':\n",
    "                print(\"No more data to retrieve\")\n",
    "                break  # No more data to retrieve\n",
    "            all_data.extend(data)  # Append the batch to the list\n",
    "            offset += batch_size  # Increment the offset for the next batch\n",
    "            print(f\"Processed {offset} records\")\n",
    "        else:\n",
    "            logging.error(f\"API request failed with status code: {response.status_code}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                print(\"Retrying in 5 seconds...\")\n",
    "                time.sleep(5)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "        retries += 1\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying in 5 seconds...\")\n",
    "            time.sleep(5)\n",
    "\n",
    "if retries == max_retries:\n",
    "    logging.error(\"Max retries reached. Exiting.\")\n",
    "\n",
    "# Save the offset to a file for resuming later\n",
    "with open(\"offset.txt\", \"w\") as offset_file:\n",
    "    offset_file.write(str(offset))\n",
    "    \n",
    "# Create a DataFrame from the retrieved data\n",
    "all_data = spark.createDataFrame(all_data)\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "all_data.coalesce(1).write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save('esports_tournaments.csv')\n",
    "\n",
    "# Save the DataFrame to Parquet\n",
    "all_data.coalesce(1).write.format(\"parquet\").option(\"header\", \"true\").mode(\"overwrite\").save('esports_tournaments.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Games Awarding Prize Money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable warnings\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "# Read the parquet file to obtain the game_id values\n",
    "parquet_data = spark.read.parquet('esports_tournaments.parquet')\n",
    "\n",
    "# Extract the game_id column values into game_ids\n",
    "game_ids = parquet_data.select('GameId').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Construct the URL for the current game ID\n",
    "api_endpoint = \"http://api.esportsearnings.com/v0/LookupGameById\"\n",
    "\n",
    "# Initialize the list to store game data\n",
    "game_data = []\n",
    "\n",
    "# Initialize parameters\n",
    "max_retries = 5\n",
    "\n",
    "for game_id in game_ids:\n",
    "    \n",
    "    # Set up the request parameters\n",
    "    params = {\n",
    "    \"apikey\": api_key,\n",
    "    \"gameid\": game_id,\n",
    "    }   \n",
    "    \n",
    "    retries = 0\n",
    "    \n",
    "\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            # Send a GET request to the API\n",
    "            response = requests.get(api_endpoint, params=params, verify=False)\n",
    "\n",
    "            # Check if the request was successful (status code 200)\n",
    "            if response.status_code == 200:\n",
    "                # Parse the JSON response\n",
    "                data = response.json()\n",
    "                # Add the GameId to the data\n",
    "                data[\"GameId\"] = game_id\n",
    "                # Append the data to the list of data entries\n",
    "                game_data.append(data)\n",
    "                # Print the status\n",
    "                print(f\"Processed game ID {game_id}\")\n",
    "                break\n",
    "            else:\n",
    "                logging.error(f\"Request for game ID {game_id} failed with status code {response.status_code}\")\n",
    "                retries += 1\n",
    "                if retries < max_retries:\n",
    "                    logging.info(f\"Retrying in 5 seconds (Retry {retries} of {max_retries})...\")\n",
    "                    time.sleep(5)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # Handle connection and request exceptions\n",
    "            logging.error(f\"Request error for game ID {game_id}: {e}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                logging.info(f\"Retrying in 5 seconds (Retry {retries} of {max_retries})...\")\n",
    "                time.sleep(5)\n",
    "                \n",
    "# Create a DataFrame from the collected game data\n",
    "game_data = spark.createDataFrame(game_data)\n",
    "                \n",
    "# Save the DataFrame to CSV\n",
    "game_data.coalesce(1).write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save('games_awarding_prize_money.csv')\n",
    "\n",
    "# Save the DataFrame to Parquet\n",
    "game_data.coalesce(1).write.format(\"parquet\").option(\"header\", \"true\").mode(\"overwrite\").save('games_awarding_prize_money.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Games Genres Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "url = 'https://www.esportsearnings.com/games/browse-by-genre'\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Find all genre titles, game statistics, and game boxes\n",
    "genre_titles = soup.find_all('span', class_='games_main_genre_title')\n",
    "genre_stats = soup.find_all('span', class_='games_main_genre_stats')\n",
    "game_boxes = soup.find_all('div', class_='games_main_game_box')\n",
    "game_links = soup.find_all('a')\n",
    "\n",
    "# Extract text and statistics as lists\n",
    "genre_titles = [genre_title.text for genre_title in genre_titles]\n",
    "genre_num = [int(re.search(r'\\d+', genre_stat.text).group()) for genre_stat in genre_stats]\n",
    "game_titles = [game_box['title'] for game_box in game_boxes if 'title' in game_box.attrs]\n",
    "game_ids = [int(match.group(1)) for link in game_links if (match := re.compile(r'^/games/(\\d+)').match(link.get('href')))]\n",
    "# games_ids = spark.createDataFrame(game_ids, columns=['Game Id'])\n",
    "\n",
    "# Initialize an empty list to store dictionaries\n",
    "data = []\n",
    "\n",
    "# Iterate through the pairs of genre titles and game boxes\n",
    "position = 0\n",
    "for genre_title, num_games in zip(genre_titles, genre_num):\n",
    "    game_titles_list = game_titles[position:position + num_games]\n",
    "    game_ids_list = game_ids[position:position + num_games]\n",
    "    \n",
    "    # Create a dictionary for each game and add it to the data list\n",
    "    for game_title, game_id in zip(game_titles_list, game_ids_list):\n",
    "        data.append({'Genre': genre_title, 'Game Name': game_title, 'Game Id': game_id})\n",
    "    \n",
    "    position += num_games\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "df.coalesce(1).write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save('game_genres.csv')\n",
    "\n",
    "# Save the DataFrame to Parquet\n",
    "# df.coalesce(1).write.format(\"parquet\").option(\"header\", \"true\").mode(\"overwrite\").save('game_genres.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esports-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
