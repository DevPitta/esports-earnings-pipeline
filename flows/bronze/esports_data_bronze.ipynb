{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "import urllib3\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/06 17:31:31 WARN Utils: Your hostname, pitta resolves to a loopback address: 127.0.1.1; using 192.168.100.7 instead (on interface enp6s0)\n",
      "24/01/06 17:31:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/06 17:31:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"esport_data_bronze\") \\\n",
    "    .config(\"spark.executor.memory\", \"64g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Recent Tournaments Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable warnings\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "# Set your API key and the API endpoint URL\n",
    "api_endpoint = \"http://api.esportsearnings.com/v0/LookupRecentTournaments\"\n",
    "\n",
    "# Load the offset from a file, or start from 0 if it doesn't exist\n",
    "try:\n",
    "    with open(\"offset.txt\", \"r\") as offset_file:\n",
    "        offset = int(offset_file.read())\n",
    "except FileNotFoundError:\n",
    "    offset = 0\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# Initialize parameters\n",
    "batch_size = 100\n",
    "max_retries = 5\n",
    "retries = 0  # Initialize the retry counter\n",
    "\n",
    "while retries < max_retries:\n",
    "    # Set up the request parameters\n",
    "    params = {\n",
    "        \"apikey\": api_key,\n",
    "        \"offset\": offset,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Make the API request\n",
    "        response = requests.get(api_endpoint, params=params, verify=False)\n",
    "        \n",
    "        # Check for successful response\n",
    "        if response.status_code == 200:\n",
    "            # Check if response content is b'' (empty bytes)\n",
    "            if response.content == b'':\n",
    "                print(\"No more data to retrieve\")\n",
    "                break\n",
    "            data = response.json()\n",
    "            # Check if data is empty\n",
    "            if not data or data == b'':\n",
    "                print(\"No more data to retrieve\")\n",
    "                break  # No more data to retrieve\n",
    "            all_data.extend(data)  # Append the batch to the list\n",
    "            offset += batch_size  # Increment the offset for the next batch\n",
    "            print(f\"Processed {offset} records\")\n",
    "        else:\n",
    "            logging.error(f\"API request failed with status code: {response.status_code}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                print(\"Retrying in 5 seconds...\")\n",
    "                time.sleep(5)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "        retries += 1\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying in 5 seconds...\")\n",
    "            time.sleep(5)\n",
    "\n",
    "if retries == max_retries:\n",
    "    logging.error(\"Max retries reached. Exiting.\")\n",
    "\n",
    "# Save the offset to a file for resuming later\n",
    "with open(\"offset.txt\", \"w\") as offset_file:\n",
    "    offset_file.write(str(offset))\n",
    "    \n",
    "# Create a DataFrame from the retrieved data\n",
    "all_data = spark.createDataFrame(all_data)\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "all_data.coalesce(1).write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save('esports_tournaments.csv')\n",
    "\n",
    "# Save the DataFrame to Parquet\n",
    "all_data.coalesce(1).write.format(\"parquet\").option(\"header\", \"true\").mode(\"overwrite\").save('esports_tournaments.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esports-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
